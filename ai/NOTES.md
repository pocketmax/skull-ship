https://stackblitz.com/edit/rxjs-operating-heavily-dynamic-uis

Goals is the pipe through which your contiousness flows
goals activate your prefrontal cortex, enhancing decision-making and focus
they reduce the negative chatter of your brains default mode network
combating worry and rumination, and most importantly, goals light up your brains reward centers
the bigger your thinking, the wider your possibility space
possibility space: refers to the full range of potential states or outcomes that a complex system can occupy
you have to set goals in order to become goal directed, but you also have to set them in order to attain them.
the diff between a dreamer and a doer is the dreamer stays stuck in the big thinking not knowing what steps to take next
Start with a desired outcome and work backwards systematicly
purpose isnt necessarily something that finds you. its something that you choose and then iterate upon as you engage with life.
high hard goal: anbitious longterm objective that takes one to 5 years to achieve (guy who ate a plane)
goal-directed action: action that will progress you more to a given goal than any other possible action conceivable
performing goal-directed actions while in flow state is the corner stone of peak performance
commit to goal directed actions
Perceived importance: the personal significance we assign to an activity
the more goal-directed your actions are, the greater the dopamine release, the more flow you will get

Divergent thinking vs Convergent thinking
Decreasing cognitive load increases ability to slide into a flow state
More strategic you are, the less often reactionary tasks will occur
End of each workday, take 10 min to plan tomorrows goal-directed actions
check daily activities against high hard goals to ensure alignment
Identify the three, most critical goal-directed actions for the following day
set wildly specific, clear goals for each of your big goal-directed actions

# High hard goal: To become an expert in AI

have an understanding of the fundimentals
Not restricted to only using 1 tool
break down tech into smaller tech and issolate
Can impliment using more then one tool or environment
Can explain fundimentals of AI at different levels from end user to expert
Can understand others explaining AI topics
Can create a basic AI from scratch without frameworks
Can understand charts and graphs of data concepts and projects
Can contribute to existing AI projects
Can fix bugs in others projects
Can understand white papers
Can have an AI conversation with an expert

# how to test growth

- GPT asks test questions
  - corrects answers
  - categorizes questions
  - scores tests

# Ongoing lists

- terms
  - definitions
  - relation to other terms
  - how it relates to AI
- equations
  - screenshot or math mark down
  - description
  - example use cases
  - how it relates to AI
  - how they relate to each other
- chat prompts
  - what they do
  - AIs that use them
  -

## sparing tasks

- refactor existing practice projects using diff tech

## reacting out

- help others with their problems on message boards
  - stack exchange
  - github issues
  - google msg boards
- contribute to github projects
-

## tiny tasks

- small enough to do in less than an hour
- can do from scratch
- repeatable to keep subject fresh

## learning resources

- books
- tutorials (articles, vids)
- white papers
- presentations

### how to break them down

- get KWs
- skill level
- AI: ask questions about material
- send questions to author

## edge learning

- DnD math learning
- data analaytics
- stock market
- math games

## aspects of tech

- math
- SDKs: keras, TF, numpy, scikit
- tools: chatGPT, Ollama, paperspace
- cloud services

## projects

- mini: can do in a few minutes with very lite env
- big
  - itterate over time
  - divide into issues
  - BDD

## tools

- an overview map of all the techs and how they are associated

- Collect list of videos I don't understand

* rate their difficulty level
* get common keywords
* write review
* note topics that went over my head

- Record myself using selfi cam and record screen

* how often I get distracted
* what I get distracted with
* level of distraction

Ramble rumble: take a topic or subject and write as much about it as possible

- bonus: avoid mentioning certain topics (someone would have to test)

Power-down ritual

1. avoid goal drift
2. Line up the dominoes
3. set clear goals
4. clear out any undone tasks

## year1 (example)

validate problem and solution
develop prottypes
secure intitial funding
Q1

---

conduct market research
identify target market
define the problem

    Month1
    ------
    Identify target market
    Conduct customer interviews
    Gather insights
      Week1
      -----
      Create a customer interview script
      Identify 50 potential interviewees

      Week2
      -----
      Conduct first 10 customer interviews

    Month2
    ------
    Analyze interview data
    identify common themes
    define a clear problem statement
    Month3
    ------
    Ideate potential solutions
    Craft unique value propositions

Q2

---

develop a prototype solution
gather user feedback
Q3

---

refine prototype
year2

---

build the team
refine the product
establish partnerships
year3

---

scale the business
sceure significant funding

GOAL STACK
Unattainable infinite purpose
goal directed actions

- weekly
- daily
- hourly
- min to min

Purpose
High-hard goals
annual goals
quarterly goals
monthly goals
weekly goals
daily goals

BUILDING YOUR GOAL STACK

1. Expand your posibility space
1. belief contagian
1. brute force big thinking
1. goal attainment and self-efficacy
1. reverse engineer your goal stack
1. set a high hard goal

RE-ALIGN

# K8S (POD)

- POD master
  - note relations to other objects
  - do next object related to a pod
- ??? master
  - note relations to other objects

# 5 money making rules

Rule 1: don't hate on money making
Rule 2: earn more than needs and wants
Rule 3: put lazy money to work
Rule 4: stack your chips and avoid the dip. save and don't spend what you save
Rule 5: stay away from broke people

Types of Timeseries models
MA: Moving Average
AR: Autoregressive
ARIMA: Autoregressive Integrated Moving Average
SARIMA: Seasonal autoregressive integrated moving average
VAR: vector autoregressive
VECM: vector error correction model
...

Categories of timeseries models
Decomposition: splits TS into categories that match certain patterns
Smooth-based: shows trends better, reduces outliers, reducing random variance
Moving-average: output var is litnearly dependent on current and various historical value of a stochastic component
Exponential smoothing: runs func of smooth based model using an exponential window function. A simple process for determining anything depending on prior assumptions made i.e. seasonality

Timeseries forcasting models based on decomposition

- additive decompisition model
- multiplicative decompostion model

Timeseries forcasting models based on MA model

- AR
- MA
- ARMA
- ARIMA
- SARIMA: an extention of ARIMA
  - handles seasonal patterns
  - handle non-stationary data
  - req large amounts of data
- VAR
- VECM
- LSTM: long short-term memory
  - handle long-term dependencies
  - handle non-stationary data

Timeseries questions to ask

- volume of data available: more data is helpful
- required time horizon of predicitions: shorter time horizon are easier to predict over longer ones
- forecast update frequency: foreacasts might have to be updated fequently
- forecast temporal frequency: forcasts can be made at lower or higher frequencies. downsampling and upsampling of data

# MATH (pre algebra)

- do all syllabus stuff

# MISC

- better layout sys than flexbox
- alt unit of measurment than px
- round robin lib in stackblitz

# AI (autokeras)

- all tutorials
- examples of all functions
- all questions answered
- understand it

# ARDRINO

- get smallest hello world kit possible and finish it

# BLOCKHUB

## a test chain to test changes against

- stocks, CRM data (linked data), images,
- excel sheet format
- csv format
- images
- invalid buffer data
- shuffled data
- continuous emit to test "walk to live" functionality

## walk to tail works

## walk to live works

## a way to sort blobs by timestamp within doc i.e. populate a db up to an exact point in time

1. identify timestamp in doc in blob
2. index doc by that TS
3. pass newly sorted docs into observable

- sync only
- things to watch out for

* skewed TS
* two docs with the same TS
  - sort by TS, blob height
* TS without TZ
  - set a default TZ
* doc with multiple TS
* doc with TS in diff places
* TS found too early or too late in chain i.e. TS from several days ago found in recent blob
  - specify window of blobs to collect. If TS is supposed to be in that range, then flag it

- how to test

* docs with TS
* docs with TS in different place
* docs with TS with extreme offset
* docs with datetime + TZ
* docs with bad datetime + TZ

sortstep observable: emits last value in sorted array as new value comes in

```js
const bla = new Subject()
bla
.pipe(
  bufferCount(20),
  map(items => items.sort((a, b) => {
    if(a.ts === b.ts) return Number(b.blobHeight) - Number(a.blobHeight)
    return Number(b.ts) > Number(a.ts);
  })),
  concatMap((v)=>from(v))
)
.subscribe((v)=>{
    console.log(v)
});
for(let i =0;i<=20;i++){
  if(i===12){
    bla.next({
      ts: i,
      blobHeight: i + 2
    })
    bla.next({
      ts: i,
      blobHeight: i + 3
    })
  }
  bla.next({
    ts: i,
    blobHeight: i + 1
  })
}
```

## steps

1. locate TS in doc
2. normalize TS (convert from date/time to TS)
3. pass into observable

on new msg in observable

1. sort array
2. pop last item

```js

let bla = new Subject()
bla.next({id,val})


bla
.pipe(
  // collect x vals
  // sort vals
  // emit last val
)
.subscribe((v)=>{

})
```

## results

array of TS+blob height

- exceptions arrays
  docs with TS out of window
  docs without TS
  docs with multiple TS

visual learner - primary
auditory learner - secondary

https://www.integrate.io/blog/etl-vs-elt/
https://www.gathr.one/plans-and-pricing/#

how to find unmatched blobs across a group of walker filters

system design primer - devops
https://github.com/donnemartin/system-design-primer

# IDEAS

## stub page: container that shows just a page

- show basic attributes
-

circle graph idea from https://youtu.be/xw83cRofkpM?si=s2nr6-HQwnMb2FLE&t=1025

- used for generic async tasks
- each color represents a stage in that task
- size represents amount of work
- location represents something
- durration of color represents something else

- rxjs operator for cascading retries

customer types by knowledge level
https://bytebytego.com/
https://my-js.org/assets/files/system_design-89ea3329b17dd21409f625db90073fb4.pdf
https://shivaram.org/publications/shivaram-dissertation.pdf

.blobs = specific for blob data
.blobs.content = the buffer
.assets
.items = generic list
.blobs
blueprint.items

BigQuery learning

sanitize/test/fix data before appending anything
assume valid data struc in queues

APPEND TO MID LOOP WITH newBlock as tailBlock

- beforeStitchSchema
  1:{ tailBlock, newBlock }
- afterStitchSchema
  2:{ linkBlock, tailBlock }
- value validation

http://symplasma.com/
https://www.blankenship.io/trimtab/
https://www.blankenship.io/trimtab/001-huseby/podcast.mp3
https://www.weboftrust.info/
https://en.wikipedia.org/wiki/OFFSystem
https://www.hyphanet.org/index.html

Dune 2 questions

- Weapon of choice - about dune
- Frank Herbert was into psychedelics when he wrote it
- Based on oil war in the Middle East
- Filmed at city of Petra where Indiana jones was filmed
- not enough cow bell
- Like…
  - Dances with wolves
  - 5th element
  - Star Wars
  - Islam
  - Christianity
  - Game of thrones

1. Why use knives in a world with guns?
2. Why can’t the emperor just bomb the planet from space?
3. What significance does the desert peoples jesus have? Pauls mother was really focused on him being that guy all of a sudden. Isn’t she supposed to be working for the space nuns?
4. Why do the sand dance? Don’t the sand worms detect any ground movement?
5. Those sand worm pounder black things don’t make sense. Why have them when any movement will attract the sand worms? Also the heavy spice machinery would easily attract the worms anyway
6. If the desert people can ride the worms then why can’t they tame them like horses so they aren’t a threat?
7. If the still suits recycle body water, why do they have a nose plug and not a mouth plug to drink from?
8. What was the point of Pauls mom being the Barrons daughter? Was that by accident or on purpose?
9. Why did the space nuns think their Jesus would be a boy when all they make are girls?
10. why do some have blue eyes but not all?
11. If exposure to the spice causes addiction, why aren't the desert people full blown addicts
12. why did the emperor give the desert planet to pauls family and take it from the baron?
13. why were they so worried about paul and his mom still being alive? It's just 2 people and they didn't know they would join the desert people
14. was zendeya related to that guy paul killed a the end of the first movie?
15. Why is riding a sandworm such a big deal for paul when a bunch of other people did it in the ending battle
16. What is the big deal about the south? Everyone keeps saying no one lives there. I know paul doesn't want to go there since he will start a war but whats in the south?
17. won't those open air pools the desert people have evaporate eventually since they aren't covered?
18. why didn't the emperors men kill paul as soon as he walked in the room? They just stood there.
19. whats the significance of the ring pauls father had?
20. When the bad guys were floating up the mountain, what was that? anti-gravity? was that something powered by spice?
21. reverend mother is a title only from the desert people. Not the space nuns right?
22. the old woman used the voice on pauls mom to drink the blue water. Does that mean the space nuns already had agents mixed in with the desert people?
23. when zendia followed him when paul was supposed to survive alone. Wouldn't that be cheating since he had help?
24. Is the prophecy of the desert people created by the space nuns?

```yaml
apiVersion: v1
kind: Deployment
metadata:
  name: myapp-pod
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    volumeMounts:
    - mountPath: /etc/rabbitmq
      name: mosquitto-config-file
  volumes:
  - name: mosquitto-conf
    configMap:
      name: mosquitto-config-file
  - name: mosquitto-secret
    secret:
      secretNAme: mosquitto-secret-file
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: mosquitto-config-file
data:
  mosquitto.conf: |
    log_dest stdout
    log_type all
    log_timestamp true
    listener 9001
---
apiVersion: v1
kind: Secret
metadata:
  name: mosquitto-secret-file
type: Opaque
data:
  secret.file: |
    sdfasdfasdfasdf
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app.kubernetes.io/name: MyApp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    volumeMounts:
    - mountPath: /etc/rabbitmq
      name: app-cfg
  initContainers:
  - name: add-enabled-plugins-file # adds enabled_plugins file to tmp vol
    image: busybox:1.28
    command: ['sh', '-c', "cat '[rabbitmq_prometheus].' > /tmp/app-cfg/enabled_plugins"]
    volumeMounts:
    - mountPath: /tmp/app-cfg
      name: app-cfg
  - name: add-rabbitcfg-file # may not need this depending on the order of mounting stuff
    image: busybox:1.28
    command: ['sh', '-c', "cat 'foobar' > /tmp/app-cfg/rabbitmq.config"]
    volumeMounts:
    - mountPath: /tmp/app-cfg
      name: app-cfg
  volumes:
  - name: app-cfg
    emptyDir:
      sizeLimit: 500Mi
```

general queues:

- ingress: (blobs)
- deployed: (blob info that have been deployed to bucket and just receipts)

corporate deploy

- web-append: 1:\*
- rabbitmq: 1
- deploy blob: 1:\*
- builder: 1
- minio: 1

corporate queues:

- ingress - raw blobs
  (is authed) q->q
- blobs.authed - authed
  (populate cfg) redis->q
- blobs.cfged - config applied
  (deploy) q->bucket->q
- blobs.deployed - blob saved on bucket with blob data replace with info about bucket
  ?????
- blobs.grouped.{chain} - size determins activity of chain
- blocks.ready - 1 tail block
  (pop blob set and merge)
  - act grouped blobs
- blocks.staged - 1 tail block, 1 blob set
  (create new block)
- blocks.newblockcreated - 1 tail block, 1 new block
  (save new block)
- blocks.newblocksaved - 1 tail block, 1 new block
  (save next field to tail block)
- blocks.nextfieldupdated - 1 tail block (next), 1 new block
  (append new block to start queue to restart the process)

* ack blob set msg

### blob-data schema: new blob with buffer data

## msg for request blob

msg blob
<buffer>
msg properties

- auth
- blobinfo (length, checksums, mime)

## msg for request info

msg blob
{
headers: {},
client: {
ip, OS ver, etc etc
},
method,
path
}
msg properties

- auth
- blobinfo (length, checksums, mime)

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "asdfasdf",
  "type": "object",
  "$ref": "#/def/RootBlock",
  "def": {
    "BeforeStitch": {
      "type": "object",
      "description": "before adding a new block to the chain",
      "properties": {
        "tailBlock": {
          "$ref": "#/def/TailBlock"
        },
        "newBlock": {
          "$ref": "#/def/NewBlock"
        }
      }
    },
    "AfterStitch": {
      "type":"object",
      "description": "after adding a new block to the chain",
      "properties": {
        "linkBlock": {
          "$ref": "#/def/LinkBlock"
        },
        "tailBlock": {
          "$ref": "#/def/TailBlock"
        },
      }
    },
    "Receipt": {
      "description": "made after blob saved to bucket. used to build blocks",
      "allOf": [{
        "$ref": "#/def/HashItem"
      }]
    },
    "Blueprint": {
      "type": "object",
      "properties": {
        "tailBlock": {
          "$ref": "#/def/TailBlock"
        },
        "items": {
          "$ref": "#/def/Hashes"
        }
      }
    },
    "BlobItem": {
      "description": "ingress object appender captured containing blob data",
      "type": "object",
      "properties": {
        "hashes": {
          "type": "object",
          "description": "key map of hashes and crypt types",
        },
        "length": {
          "type": "integer",
          "minimum": 1
        }
      }
    },
    "RootBlock": {
      "type": "object",
      "required": ["chain","id","prev","height","hashes"],
      "additionalProperties": false,
      "properties": {
        "chain": {
          "$ref": "#/def/Hash"
        },
        "id": {
          "$ref": "#/def/Hash",
        },
        "prev": {
          "$ref": "#/def/Hash",
        },
        "next": {
          "$ref": "#/def/Hash",
        },
        "height": {
          "type": "integer",
          "minimum": 0,
          "maximum": 0
        },
        "blobs": {
          "$ref": "#/def/Blobs",
        }
    },
    "LinkBlock": {
      "type": "object",
      "required": ["chain","id","prev","next","height","hashes"],
      "additionalProperties": false,
      "properties": {
        "chain": {
          "$ref": "#/def/Hash",
        },
        "id": {
          "$ref": "#/def/Hash",
        },
        "prev": {
          "$ref": "#/def/Hash",
        },
        "next": {
          "$ref": "#/def/Hash",
        },
        "height": {
          "type": "integer",
          "minimum": 1,
        },
        "blobs": {
          "$ref": "#/def/Blobs",
        }
      }
    },
    "TailBlock": {
      "type": "object",
      "required": ["chain","id","prev","height","hashes"],
      "additionalProperties": false,
      "properties": {
        "chain": {
          "$ref": "#/def/Hash",
        },
        "id": {
          "$ref": "#/def/Hash",
        },
        "prev": {
          "$ref": "#/def/Hash",
        },
        "height": {
          "type": "integer",
          "minimum": 1,
        },
        "blobs": {
          "$ref": "#/def/Blobs",
        }
      }
    },
    "NewBlock": {
      "type": "object",
      "required": ["chain","id","prev","hashes"],
      "additionalProperties": false,
      "properties": {
        "chain": {
          "$ref": "#/def/Hash",
        },
        "id": {
          "$ref": "#/def/Hash",
        },
        "prev": {
          "$ref": "#/def/Hash",
        },
        "blobs": {
          "$ref": "#/def/Blobs",
        }
      }
     },
    },
    "Hash": {
      "type": "string",
      "pattern": "^[a-f0-9]{32}$|^[a-f0-9]{64}$"

    },
    "Blobs": {
      "type": "array",
      "minItems": 1,
      "items": {
        "$ref": "#/def/BlobItem"
      }
    },
    "BlobItem": {
      "type": "object",
      "properties": {
        "hash": {
          "$ref": "#/def/Hash",
        },
        "crypt": {
          "description": "crypt func used i.e. md5",
          "type": "string"
        },
        "length": {
          "type": "integer",
          "minimum": 1
        },
      }
    },
  }
}
```

```js
let bla = new Proxy({
  maxGroups: 10,
  harvest: (key, list)=>{   // runs when list is ready to harvest
    broker.append(key + '_groups',list)
  }
})
  let ttl = {
    'mykey': 6000
  };
  let updateTS = {
    'mykey': 234234234
  };
  let val = {
    'mykey'=[<buffer>]
  };

})
bla.cfg('mykey',{min: 4, max: 15, ttl: 300})
bla.add('mykey',<buffer>) // returns false if max groups met
```

bla = new MsgBucket(key: str, min: 2, max: 15, ttl: 300)
bla.append(msg)

store.LinkBlocks(tailBlock, newBlock): sets next field in tailBlock to newBlock and saves tailBlock in bucket

- expects only diff in tailBlocks is next field
- Assumes both blocks are in bucket
- check newBlock exists before finishing
- fail: something went wrong
- complete: success

# GROUPING SYS: key/val goes in, groups come out

init: in-queue(item),out-queue(items),max=50,ttl=2000
stats: in items/sec, out groups/sec, out items/sec

- vanilla
- groups on key
- value is blob
- no duplicate groups or items in groups
- track total ingress,egress msgs
- routes to sub queue
- routing to
- worker tracks how

# HTTP CHAIN LISTENER: plugin that appends a http request body to the chain and walks the chain listening for the response with a matching transaction ID

- should be sdk agnostic. i.e. same code works with multiple http servers

# LIST FOREST SYS: lists of groups of hashes

init: in-queue (key/val), redis instance, http api
stats: in items/sec, out items/sec

## adding entries

1. pop from in-queue
2. append to new/existing list

## using entries

1. pop from a list with a given key

# BLUEPRINT BUILDER SYS: builds bp for multiple chains at scale

init: in-queue (tail), list forest api, out-queue (bp)
stats: in items/sec, out items/sec

1. pop from in-queue
2. pop item from list forest
3. build bp with tail + list group
4. append bp to out-queue
5. delete group from cache

- appends to out-queue
- builds 1 block at a time per chain
- note: redis SET has flag to block overwrite

# BLOCK HYDRATOR: fetches blobs by hash in a block

- knows block schema

## funcs

- getBlobs(block): array of blobs in block order

# WALKER SYS: walks multiple chains at scale

init: buckets[]
gettail(from,to): next: blocks, comp

- each walk is a query that clients use to get a feed of blocks
- knows block schema
- step

```json
{
  "store": {},
  "retries": 2,
  "ttl": 200,
  "block": "...",

}
```

## funcs

- query(from,to): stream of blocks
- query...
  - from/to blocks (tail or live)
  - sync/async
  - store bucket urls
- client...
  - sends query to service
  - controls retry attemps
  - controls TTL
  - receives stream of blocks
  - can early kill query before complete (need to figure this one out)
  -
- worker...
  - pops block from queue
  - pulls next block from bucket
  - appends next block to queue for another worker to pop
  - stream blocks to client
- use cases
  - walk sync domain chain to build table of active chains
    1. send query to walk from root to live
    2. (blocks streaming)
    3. hydrate blocks with blobs
    4. parse blobs in chain order
    5. send CRUD to DB
  - building blocks sync
  - walk sync domain chain to build table of active users for authentication
  - walk async all chains to verify blocks
  - walk tail range to delete prev blocks across all chains
- features
  - breakup a query into split queries if possible
  - can scale like crazy

* get height of from,to blocks
* calc height split ranges (1-100 in 4 groups = 1-25,26-50,51-75,76-100)

- optimize some how by skipping known blocks
  totail(from,to): streams blocks, completes (async/sync done on client)
  livetotail(from): streams blocks (async/sync done on client)

# ACTIVE CHAINS TABLE: list of tables that should be active and running

- chain ID
- tail block
- tail block updatedAt

1. worker pops block
2. if next field, loop with next block
3. if !next field, write block to table
4. loop tail

modifiedAt
createdAt
updatedAt
deletedAt

# SLIDING WINDOW SYS: only keeps latest x blocks per chain. used for bucket that verifies next field to keep it from filling up

init: in_queue (tail), tail length

1. on tail: walk backward until height = height - length
2. toRoot(blockId)

- short TTL
- complete on timeout
- add cb that deletes the block after fetching

# BLOCK BUILDER: builds blocks from blueprints. outputs tail block

init: in-queue, out-queue
stats: in items/sec, out items/sec

- pops blueprint from in-queue
- appends tail block to out-queue

# DEPLOY TAILS: sends tail blocks to buckets (new object)

init: in-queue, buckets

# NEXT FIELD SYS: researches block pairs and sets next field from prev block to tail block

init: in_queue (prev,tail), out_queue (receipt), bucket

1. verify both blocks haven't changed in bucket
2. set next field in prev block
3. save prev field to bucket

# CONFIG TYPES

- stand alone: 1 container, 1 chain
- blend: split between local and cloud
  - some blobs go to cloud
  - some blobs hashes go to cloud
  - cloud only builds blocks
  - buckets can be first party or third party
- cloud: service
  - everything done in cloud

# STAND ALONE CONFIG

## run curl cmd on loop

while sleep 1; do curl -H "Accept: application/json" -H "Content-type: application/json" -X POST -d '{"id":100}' http://localhost:1300/bla; done

## DEAD LETTER QUEUE: everything wrong goes to DLQ

1. worker pops msg from queue
2. gets error code and meta data
3. pass error meta data to repair func for that error code

- repair func success: append one or more msgs to one or more queues
- repair func fail: output error to log and continue

## QUEUES

main (incoming http requests)
blobs (blobs ready to be saved to blobs bucket)
checksums (receipts of checksums)

## ENV

STORE_ACCOUNT_ID
STORE_ACCOUNT_KEY
BROKER_USER
BROKER_PASS

## CLI (init mode) creates/deploy root block and returns chain ID

- store
- bucket / block-bucket
- init-root-block

## CLI (daemon mode)

- chain
- store
- min-items: min hashes per block
- max-items: max hashes per block
- ttl-items: timeout while fetching hashes
- bucket = (blob-bucket / block-bucket)
- blockkey = {hash}.block
- blobkey = {hash}.blob
- broker
- token (optional) if no value set, a random val will be set
- in-queue
- out-queue
- err-queue

## deploy blob

```js
consume('from-queue'(msg)=>{
  store.put(msg.key,msg.content)
  msg.content = '200 ok'
  append('to-queue', msg,msg.properties)
})
```

## init checklist

```js
let buckExists = ()=>interval(1000).pipe(map(()=>'bucket found!'),take(1))
let brokerConnect = ()=>interval(1500).pipe(map(()=>'broker connected!'),take(1))
let queueExists = ()=>interval(1600).pipe(map(()=>'queue exists!'),take(1))
let foundTailBlock = ()=>interval(1800).pipe(map(()=>'found tail!'),take(1))
let openHttp = ()=>interval(2000).pipe(map(()=>'open HTTP!'),take(1))
of(
  buckExists(),
  brokerConnect().pipe(
    concatMap((v)=>queueExists()
      .pipe(
        map((f)=>[v,f])
      )
    )
  ),
  foundTailBlock(),
  openHttp()
)
.pipe(
  combineLatestAll()
)
.subscribe(x => console.log(x))
```

# REDIS FOR GROUPING MSGS

- msgs treated as generic
  worker appends msg to array

1. creates array if it doesn't exist
2. appends item to array
3. updates updatedTS val for array
4. compares array length to cfg array limit
   worker checks if array ttl reached to harvest
5. for each array
6. comapres now to updatedTS to see if array is ready to harvest

tail block
asset list

---

## new block

1. upload new block to bucket
2. update next field in tail block

---

append new block to out queue

# (blob src cli args)

- db (eph)
  - row changes via trigger. Figure out how to send over net
  - on init (optional)
    - table fetch
    - schema fetch
  * username
  * password
  * db
  * trigger (if not present, output SQL to generate)
  * url
    - host/port
    - path

# detect db changes

- via triggers

# detect fs changes

- check mod/crt date on timer loop
- detect fs changes (if available)

# detect s3 changes

- tap into bucket events
- list objects and check date ts (if available)

# db-agent:

- static mode: run once
  - dump a table
- daemon mode: run live
  - dump changes in a table
- dump query results (on a timer too)

# http-agent:

# fs-agent:

# cloud queues

ingress (blob from source)
authed (authenticied blob)
assets (list of assets)
identified (owner meta added)
shared-checksumed (blob checksum added)
shared-deployed-assets (blob save to bucket, swap with receipts)
staged-blocks (array of asset receipts and one block)

dead-letter

# msg schemas

- owner meta obj
  {
  hashFn: str i.e. md5, sha256
  minAssets: 1 min count of assets for a block
  maxAssets: 20 max count of assets for a block
  assetTTL: 60 timeout to wait for assets to build array in seconds
  }
- error msg obj
  {
  error: {
  code: str i.e. err_height_invalid
  path: str path in msg the error is about i.e. foo.bar.id
  msg: str error msg
  }
  fromQueue: str queue the msg was on when error was found
  msgEncode: base64,
  msg: original msg encoded in base64
  }

# msg schema validation

- ignore exta keys

```json
{
  "title": "Product",
  "description": "A product from Acme's catalog",
  "type": "object",
  "properties": {
    "auth": {
      "description": "auth info",
      "type": "object"
    },
    "createdTS": {
      "description": "timestamp - updated on creation",
      "type": "int"
    },
    "updatedTS": {
      "description": "timestamp - updated every time obj changes",
      "type": "int"
    },
    "chain": {
      "description": "chain the msg is for",
      "type": "string"
    },
    "owner": {
      "description": "owner meta data",
      "type": "object"
    },
    "assets": {
      "description": "list of array recipts. no blobs",
      "type": "array"
    },
    "blob": {
      "description": "owner meta data",
      "type": "object",
      "properties": {
        "encode":{
          "description": "encode type i.e. ETF-8",
          "type": "string"
        },
        "length": {
          "description": "size of blob",
          "type": "number"
        },
        "content": {
          "description": "buffer encoded in base64",
          "type": "string"
        }
  }
}
```

{ STARTER MSG
auth: {}
createdTS: num
updatedTS: num
chain: str
blob: {
mimeType: str
encode: str
length: num
content: <blob>
}
{ ARRAY OF ASSET RECEIPTS + TAIL BLOCK
auth: {}
createdTS: num
updatedTS: num
chain: str
owner: {}
tailBlock: <block>
assets: [{
encode: str
checksum: str
length: num
receipt: {}
},{
...
}]
}

DEPLOY (replace) tailBlock TO BUCKET
{
auth: {}
createdTS: num
updatedTS: num
chain: str
owner: {}
tailBlock: {
next: str
}
newBlock: {
height: num
}
}

1. ingress blobs
2. check auth
3. fetch owner latest obj used when assets merged later on
4. gen-checksum (gets hash func from owner meta)
5. deploy-asset
6. stage-block (fuses array of assets with a block)
7. build-block

Have to

CLI data types

- url
  - S3 (expect just host)
  - S3all (expect bucket and key)
  - redis
- hash
- taghash

foo = new Foo(driver, inq, outq)
foo.call((msg){
})
fetches from in-queue, appends to out-queue

- return error object to signal error with error queue as field in it
- return anything else to append to out queue

# web to msg: captures web content and appends to queue as 2 msgs...body blob and everything else with a ref to body blobs checksum

- http src -> queue
- scalable
- used in enterprise and free tier
- generic that isn't blockchain specific
- doesn't block when enqueueing msg
- x-token to block certain traffic

## INIT

1. connect to broker
2. test in-queue exists
3. test err-queue exists
4. start http server
5. open http port

## READY

1. recieves http request
2. append body to queue


    - add meta for length
    - add meta for mime type

3. append request doc to queue


    - add meta for length
    - add meta for mime type
    - add hash to blob in body key

- logic

```js
if(cli.token && cli.token != request.headers.token) throw 'token invalid'
let meta = cli()
myBroker.outqueue.append(requestbody, meta)
requestmeta.body='checksum from body'
myBroker.outqueue.append(requestmeta, meta)
```

# grouper broker: manages grouper instances

- based on queue size
- decides what instance goes to what queue
- track msgs/sec/queue from proxy worker that sends out msgs
- identify queues popularity
  - msgs/sec
  - track appends per sec from process that appends msgs
- service discovery
  - builds db of queues by walking domain chain
  - scans broker for queues

# grouper:

- 1 queue per worker
- configed with min/max/ttl item limits
-

queue-to-queue
pop-to-many

# shoveltomany: pops from in-queue and appends to out-queue or dynamic queue if template tag in out-queue detected

- queue -> dyno out-queue
- scalable
- used in enterprise
- generic that isn't blockchain specific
- docker image can be extended to include a lambda that returns bool if it should pass to out-queue

1. pops msg from in-queue
2. defines out-queue name based on msg and cli out-queue arg
3. appends to out-queue

- cli args
  - broker: amqp://host
  - in-queue: foo
  - out-queue: {msg.meta.my-field}\_chain
  - err-queue: bar
- logic

```js
let msg = myBroker.inqueue()
let outqueue = parseCli('out-queue', msg)
myBroker.outqueue.append(outqueue, msg)
```

queue-to-bucket
queue-dump
pop-to-bucket
msg-to-obj
queue-to-bucket

# msg to object: pops msg from in-queue THEN sends to bucket as object

- blob src -> bucket
- scalable
- optionally append completion receipt when done to out queue

1. pops msg body from queue
2. sends msg body to bucket (can extra confirm it's there)

- confirmed done when: bucket write returns success
- out schema: msg batch
- args
  - broker: amqp://host
  - in-queue: foo
  - out-queue: (optional) sends receipt of object upload but not object contents
  - err-queue: bar
  - rewrite: bool
  - store: s3://host/bucket/my-path/{msg.meta.foobar}.blob
- logic

```js
let msg = myBroker.inqueue()
if(cli.overrite) {
  store.save(store.fullpath,msg.body)
} else {
  store.newobject(store.fullpath,msg.body)
}
```

setup chains

# setup chains: sets up new chain in domain as well as destroys chains

- chain -> domain
- blockchain enterprise specific
- walks domain chain for chain info

* adds/deletes rabbit queues
* ...

many-to-keymap

# msg merge: merge multiple msgs from multiple src queues into 1 keyed object msg

- queue -> queue
- chain exclusive
- v2: can group by msg meta field internally. Once batch is filled, its appended to out queue
- generic that isn't blockchain specific
- tags src msg as key in trg msg
- ack all src msgs after appending to trg queue
- merge all meta data across src msgs and apply to trg msg
- min 1 msg from all src queues
- ingress msg <any>
- egress msg {
  foo: {
  msg: <msg>
  meta: {}
  }
  bar: {
  msg: <msg>
  meta: {}
  }
  ...
  }
- args
  - broker: amqp://host
  - in:_: src queues. _ = tag name in trg msg obj
  - in:\*:ttl: config for specific query
  - out-queue: bar
  - max-size: max size in bytes of target msg
  - ttl: timeout to wait for new msgs from all src queues
  - err-queue: dead-letter

msg grouper

- generic
- groups on meta keyval

# grouper: pops msgs from one queue, groups them and adds the grouped msgs to another queue all based on meta key

- queue -> queue
- scalable
- cli
  - broker: amqp://host
  - in-queue: foo
  - group-by: msg.meta.key
  - out-queue: {msg.meta.key}\_chain
  - err-queue: bar

# builder: pops msg from in-queue, builds block and saves to store and appends new block back to queue. also creates root block with diff cli args

- queue -> bucket -> queue
- scalable
- uses next field update to prevent forks
  - assumes block without next field is tail
- in-queue msg
  {
  <block>
  }
- confirmed done when: bucket write returns success

1. pop msg (tail block) off in-queue
2. pops many msgs off side-queue
3. builds new block
4. deploy new block store.savenewobject(key,val) (if exists, throw error)
5. set next field to prev block store.newkeyval('objectkey','key','val') (ANYTHING wrong, throw error)
6. ack msgs from side-queue and msg from in-queue
7. append tail block to in-queue

- errors
  NEXT_FIELD_IS_SET
  OBJECT_EXISTS
  OBJECT_INVALID_MIME
- args
  - chain-cfg: redis://host
  - broker: amqp://host
  - in-queue: foo
  - side-queue: {msg.meta.chain}\_chain
  - err-queue: bar
  - store: s3://host/path/{msg.meta.blockid}.block
- args - deploy root block. deploys root, displays root block, exit 0
  - create-root
  - store: s3://host/path/{msg.meta.blockid}.block
- logic

```js
let tailBlock = myBroker.inqueue()
let owner = {} // check for owner field (msg.doc,msg.meta,queue.meta,cli)
let sideMsgs = myBroker.sideQueue(owner)
if(!sideMsgs) return false // early exit - no asset msgs to build block with
let newBlock = buildBlock(tailBlock, sidemsgs)
store.newObject(`${newBlock.id}.block`,newBlock) or throw 'OBJECT_EXISTS - object already exists'
store.setNewField(`${tailBlock.id}.block`,'next',newBlock.id)
.catch((err)=>{
  // NEXT_FIELD_IS_SET
  // OBJECT_EXISTS
  or throw 'NEXT_FIELD_IS_SET - next field already set'
})
broker.inqueue.append(newBlock)
```

# a way to pop multiple msgs and ack them after logic run

1. pop from main queue
2. pop from side queue in loop up to max count
3. pass array of msgs to cb

```js
// collect batches of assets to build blocks with


broker.get()


{
  min: 2,  // min msgs to pop
  max: 10, // max msgs to pop
  ttl: 100, // wait time until next msg
}
// if policy failed, nack pending msgs
// runs cb only if policy satisfied
batchConsume('my-queue',policy,(msgs, fn)=>{
    fn(msgs[0]).body  // get body for one msg
    fn(msgs[0]).ack() // act one msg
    fn(msgs[0]).nack() // act one msg
    fn(msgs).body  // get body array for all msgs
    fn(msgs).ack() // act all msgs
    fn(msgs).nack() // nact all msgs
})

```

# s3-proxy: passes s3 requests to one of many s3 stores and handles back sync

- can chain behind another proxy
- targets are labeled i.e. A,B,C
- write options
  - all
  - %
  - first x ok - broadcast
  - first x ok - round robin
  - partition key
  - shard
  - shard + parity
- read options
  - all
  - first x ok - broadcast
  - first x ok - round robin
  - partition key
  - shard
  - shard + parity
- back sync policy (put object on bucket(s) it's supposed to be on)
- env vars
  S3_A_ACCESSKEY
  S3_A_SECRETKEY
  S3_B_ACCESSKEY
  S3_B_SECRETKEY
- args
  - store[a]: s3://hosta
  - store[b]: s3://hostb
  - store[c]: s3://hostc

1. builder pops off in-queue for block
2. builder pops off dynamic queue for 1 or more asset msgs
3. builder builds block
4. builder uploads new block
5. builder updates next field in prev block

# QnA

- can a pod run a container as a daemon?
- how to augment dumped msgs in dead letter queue with error info? Add meta fields for errors. could also allow for an exchange to forward msg to an error specific queue
- can rabbit auto delete empty queues
- can sqs auto delete empty queues
- where to save chain cfg to track how many asset IDs per block
- how to handle capturing asset meta data
- how to handle sensative data
- how to handle leaving blob where it is and just capturing hash
- how to scale to multiple chains
- where to gen blob hash
- how to ack msgs like hash IDs and blocks

(blob src) -> blob agent -> [blob queue] -> asset broker -> asset bucket

blob queue ->

rabbit tasks

- add prometheus support via plugin
  - emits continuous data
- build a 2 node rabbit cluster

real world applications

- web analytics
- general pipeline processing
  - order processing

demo applications

- chat
- fs revisioning
- process orchestration with multiple nodes (voting pattern)
- lock handling like zookeeper
- many nodes processing a large asset i.e. csv
- key/val store like etcd
- ID handling
  https://enterprisecraftsmanship.com/posts/cqs-with-database-generated-ids/

problems it solves

- source of truth. no more different answers to the same reporting questions (walk chain to come up with same result)
- temperal data. see snapshots of your data at different points of time (re-walk the chain)
- no vendor lockin. use anything to store, process and view data
- managing backups (no more static backups)
- scalable reads (deploy more copies of chain)
- tampering detection (hash calc)
- can scale processing data (multiple workers)
- helps with compliance (HIPPA, PCI, etc etc)
- easier data wherehousing
- never overwrite data
- never miss an API call (capture webhooks)

Utility

- Create chain
- Walk chain
- Append msg to chain
- Verify chain
- enforce deployment policies
- dump list of asset IDs to screen
  - over a certain size
- dump list of block IDs to screen

SDK: walker

- JS, PHP,...

# read from chain

- on shutdown
  1. output current hash cache so they can be later imported (vol mount folder, put hashes in .csv file. When imported, delete file)
- runtime - incomming blob

1. gen blob hash
2. sends blob to asset store
   a. infinite retry - output warnings/error msg
3. sends hash to broker queue and cache
4. walk to live...for each new block, if hash in cache, clear it from cache
5. every 5 minutes, re-append cached hashes to queue

- solves...
  - less complex clients. only have to send blob
  - more control over what gets added to chain. Abstracts core logic away from client control

## asset meta data idea

- add meta data to asset IDs in blocks
- faster walking
- prevent fetching assets too large
- not apart of checksum calculations for block ID
- optionally added per block via appender
  - specify which fields in http header. If specified, verify value
- keys
  - hash: hash func used to make the checksum
    - fetch blob and test with multiple hash funcs
  - content-type
    - fetch blob and run mime func on it
  - urls: array of possible locations to blob in full URL format
    - scan dir and
  - length (in bits)
    - walk chain, fetch blobs
  - height: asset count from beginning
  - deleted: indicates the blob used to exist but was deleted
    - can ignore or obey these when walking
- block keys

  - skip {
    100: hash of 100th block ID. that block has the hash of the block thats 100+ away etc etc. root block has first val
    ...
    }

- VIA CLI

  - ip restrictions
    - if only ip-allow, block everything else.
    - If ip-allow AND ip-block, block everything else
      --ip-allow=999.999.999.999
      --ip-allow=192.168.3.255/12
      --ip-allow=192.12-168.1-30.22
  - mime restriction: if present, only allow blobs of these types
    --mime-allow=string ()
    --mime-block=string ()
  - hash type (defaults to md5)
    --hash=md5
    --hash=sha256
  - auth0 authentication (optional)
    --auth0
  - dry run mode
    --dryrun
  - asset store(REQ): to send blobs to
    --store=http://mystore.com
    --store=mystore.com
    --store=s3://mystore.com
  - msg broker: to send hashes to
    --broker=amqp://myhosta
    --broker=amqp://myhostb
  - queue(REQ): to append msgs to
    --queue=myqueue
  - body size limit
    --max-size=4kb
    --max-size=4t
    --max-size=4g
    --max-size=1.2gb
    --max-size=2315452343

- HTTP HEADER
  - sensative content checks

--store=http://blablabla
--store=http://blablabla
--store=http://blablabla
--store=http://blablabla
--store=http://blablabla
--store=http://blablabla
--store:write=1/2

WALKER - generic chain walker
BLOCK STORE - holds blocks (S3, FS, SFTP, FTP)
ASSET STORE - holds assets (S3, FS, SFTP, FTP)
BUILDER - builds blocks and deploys to BLOCK STORE
MSG BROKER - queue manager (rabbit, SQS)
APPENDER - appends hashes to msg broker

- restrict by IP (for webhook events as an example)
- throttle incomming blobs
- can warn/block sensative info (can be toggled off)
  - SSN
  - CC numbers
  - private keys
  - etc etc
  - detect blob contents and scan within it i.e. CC numbers in log files
- optional authentication
  CHAIN UTILITY - general chain utility
- create chain - create root block and uploads to block store
- list chains - list all chain IDs on block store
- walk chain - walks chain making sure all blocks are there
- walk chain live - walk chain
- search assets - search in asset blobs
- verify assets - walks chain and verifies asset hash matches block
- verify chain - walks chain verifying all hashes in blocks
- get block info - show info for a single block
- get asset info - show info for a single asset
  - show mime type
  - show meta data of blob if available
  - if it can't be displayed, put <blob> mark instead

https://www.golinuxcloud.com/sftp-chroot-restrict-user-specific-directory/
https://www.ibm.com/support/pages/how-restrict-sftp-user-read-only-access#:~:text=An%20sftp%20user%20or%20group,only%20have%20read%2Donly%20access.

proxy write policy

- static: i.e. min 2 or min 5
- majority (51%)
- super majority (2/3)

proxy read policy

- race: req all nodes, use first resp
- hash: get from match nodes
- broadcast

chain browser UI

- can enter S3 urls / auth
- builds to a min js file
- queries are cached
- queries run in parallel
- queries run in web workers
- queires are either not started, stopped, running, finished or live
- fetched blocks and assets are saved in local indexdb

1. vue gen hello world app
2. fetch a block
3. walk chain to tail
4. get tail

components

- StoreConfigPanel: store settings (hidable)
  fieldset group for block stores
  fieldset group for asset stores
  fields...

  - url
  - username
  - password
  - bucket (auto populates from url if available)
  - ttl
  - attempts

- queryPanel: where queries are entered
  fields...

  - chain field: for chain hash
  - from field: leave blank for head
  - to field: leave blank for live
  - stop on tail field: checkbox (only shows if to field blank)
  - verify: checksum verify blocks

  fields:settings...

  - ttl: timeout for query (overrides store ttl)
  - attempts: retry attempts. defaults to infinite (overrides stores attempts field)

  queries examples...

  - from(),to(),tailstop: emit blocks
  - from(id),to(id): emit blocks then stop
  - from(id),to() live feed of blocks

- docQueryPanel: where doc queries are entered
  fields...

  - query: query to use
  - limit: stop search after x results found

- resultsPanel: generic panel that lists results

  - headerSubPanel: summary info about last search
    - block match count/total
    - asset match count/total

  * infinite scroll
  * cache locally
  * blocks are in chain order

- blockRow: item in resultsPanel that rep a block
  - numbered icons rep assets. mouse over or tap to show ID
  - static height
  - S3 store block was fetched from
- chainRow: item in resultsPanel that rep a chain
  - block count for chain
  - chain hash
  - static height
- docRow: doc query results from an asset
  - block ID
  - asset ID (asset order # in block)
  - the result

startId: block ID to start on
stopId: optional block ID to stop on

store: url
store: {
user:
pass:
host:
bucket:
}
store: [url, url]
store: {
mode: cascade
urls: [url1, url2]
}

- retry: 10
- ttl: 1200

- iCfg
- mCfg

toTail: Tail then complete (tail as last emitted val)

- completes
- Emits all blocks + tail
  toLive: Tail then continue
- never completes
- emits all blocks
- never emits tail
- onTail: func \* Called once per method call
  getTail
- completes
- emits only tail block
- calls toTail and returns tail block

# DONE - parseurl tests work

1. parseurl.s3.test.js works
2. parseurl.fs.test.js works

# TODO - driver tests work

1. driver.s3.test.js works
2. driver.ampq.test.js works

# TODO - store tests work

1. store.test.js works

- TODO - refactor .connect out of codebase

## end results

- driver methods run without hard coding .connect call first

# TODO - make testroot blockutils unit tests

blockutils.testroot.test.js

1. make init unit tests

## end results

- testRoot method is fully tested

# TODO - make stub s3 driver unit tests

init.s3-stub.driver.test.js
marble.s3-stub.driver.test.js

1. make init unit tests
2. make marble unit tests
3. make marble value unit tests

## end results

- driver has init unit tests
- driver has marble unit tests
  - uses ID to control ttl
  - uses ID to control pass/fail
- driver has marble value unit tests

# TODO - make s3 driver unit tests

init.s3.driver.test.js
marble.s3.driver.test.js

1. make init unit tests
2. make marble unit tests
3. make marble value unit tests

## end results

- driver has init unit tests
- driver has marble unit tests
- driver has marble value unit tests

# TODO - refactor store plugins

1. refactor service logic in drv to plugins
2. make a test stub service plugin
3. drv supports test flag to use test service stub
4. get rid of other drv files and just have 1 drv file with multiple plugins

## end results

- everything uses one driver file
- driver dynamicly picks plugin based on url (including test)

# TODO - integrade test key IDs

1. test store plugin detects and parses key IDs
2. sets maxAttempts and delay based on test key ID

- test plugin tracks call attempts and response delays
- TEST IDS
  BAD04A6000FFFFFFFFFFFFFFFF
  - max attempts 4
  - delay 6000ms

## end results

- when a test ID is given, it changes the behavior of the test service plugin (max attempts and delay)

# TODO - marble unit tests

1. make bla test file with marble diagrams
2. add support for drv store + test store plugin

## end results

- can test a store method with a marble diagram using test key IDs that alter the stores behavior

```js
let TestStub = function(){
  this.put = (c)=>{
    return new Observable((sub)=>{
      sub.next(true)
      sub.complete()
    })
  }

}

if(test){
  let stuba = new TestStub() // uses observable
} else {
  let stubb = new S3Stub() // wraps an observable
}

driver.put = (c)=>{
  let attempts = 0

  // if c.id is a test hash, parse and apply config
  let maxAttempts = 4
  let delay = 200

  return stuba.put()  // either error or complete
  .pipe(
    retryWhen((obs)=>{
      obs.pipe(
        tap(v => console.log(`Value ${v} was too high!`)),
        delay(1000)
      )
      attempts++
    }),
    map(()=>{
      return {
        action: 'PUT',
        attempts
      }
    })
  )

}
stuba.connect() // observable
stuba.delete() // observable
stuba.get() // observable

let stubb = new Test()
stubb.connect() // emits
```

## store driver files

- a bla test file that uses a store and tests one of its methods

## store proxy files

## walker

# TODO - clustering

- mirror = goes to all
- shard + replicas = goes to a subset of nodes + replicas
  - on error and failed retry attempts, tries each node in cluster until one emits complete
  - option: if object found on non-shard node, try to save it to correct node(s) and delete from non-shard node
- builder can use clustering policy to deploy
  - mirror
  - shard + replicas

# TODO - mongo store driver

- connect() tests schema exists and is valid
- how to provide the user the db schema to setup?
  - on connect attempt, if schema doesn't exist, output the noSQL
- needs to support sharding i.e. only store blocks that start with 'A'
- mongodb url mongo://username:password@hostname:2171999/db/collection

# TODO - mysql store driver

- connect() tests schema exists and is valid
- how to provide the user the db schema to setup?
  - on connect attempt, if schema doesn't exist, output the SQL
- needs to support sharding i.e. only store blocks that start with 'A'
- mysql url mysql://username:password@hostname:3306/db/table
  blob table

* id
* blob
  block table
* id
* prev
* next
* height
* asset_ids

# TODO - heartbeat

## heartbeat auto runs based on detected CLI args

1. startup module that loads hb when CLI args detected

- heartbeat=url with chain
- asset-store=url
- chain=hash (if present, not in heartbeat url)

2. module emits AFTER services connect
3. echos heartbeat info on connect and ping

# TODO - support for queue broker (app/queue-broker)

1. live walk chain for trigger msgs
2. collects trigger msgs
3. at tail, figure out which queues need to be made
4. create queues on msg broker (console out)
5. emits response msg (generic msg about creating a queue on a msg broker)

## end results

1. creates queues on msg broker based on chain msgs
2. I can create a msg on the chain with the utility and see it create the queue in rabbit
3. outputs activity to console

# TODO - building with multiple chains (app/builder-broker)

1. live walk chain for trigger msgs
2. collects trigger msgs
3. at tail, figure out which builders need to be made
4. launch builders
5. after tail, on each trigger msg, update cache
6. launch builders on a timer

# TODO - cluster support for stores

- config in call to StoreFact

* mirror (default)

```js
race(obs1, obs2, obs3)
.subscribe({
  next: winner => console.log(winner),
  complete:()=>console.log('complete!')
});
```

- reads: first to complete
  1. broadcast to all
  2. complete when first complete

```js
of(obs1, obs2, obs3)
.pipe(zipAll())
.subscribe({
  next: winner => console.log(winner),
  complete:()=>console.log('complete!')
});
```

- reads: first to complete
  1. broadcast to all
  2. complete when first complete
- writes: all complete
  1. broadcast to all
  2. complete when all stores complete
- shard
  - reads: first to complete
    1. shard nodes
    2. rest of cluster
  - writes: >= %75
    1. shard nodes
    2. rest of cluster
  - back write:
    1. make sure blocks are on correct nodes
- needed observable patterns
  - val from first stream to complete
  - val when all streams complete
  - val when subset of streams complete
  - track which streams completed

# msg examples...

{ METHOD CALL
"createdAt": 234234234,
"from": "...", node the msg originated from
"ttl": 34234, max response time allowed to process msg
"maxRetries": 4,
"method": "subtract.foo",
"params": {"sub": 23, "min": 42},
"transactionId": "UUID" <- transaction ID
}
{ METHOD RESPONSE
"createdAt": 234234234,
"from": "...",
"respTime": 232342,
"retryAttempt": 3, <- the actual retry attempt the exec happened on
"result": 19, <- results of method call
"assetId":"blockID.assetID of method call ID",
"transactionId": "UUID" <- transaction ID of
}
{ NOTIFICATION
"createdAt": 234234234,
"from": "...",
"method": "app.heartbeat.ping",
"params": {
"app": "builder",
"completedBlocks": 34234
}
}
{ NOTIFICATION
"createdAt": 234234234,
"from": "...",
"method": "broker.rabbitmq",
"params": {
"action": "queue-created",
"queue": "dsfwtsdfwefwef"
}
}
{ ERROR EXAMPLE
"createdAt": 234234234,
"from": "...",
"error": {
"code": -32601,
"message": "Method not found"
},
"id": "1"
}

## minio (S3) https://hub.docker.com/r/bitnami/minio/

sudo docker run --restart unless-stopped -d --name minio --env MINIO_ROOT_PASSWORD=????? --publish 9000:9000 --publish 9001:9001 --volume /data/minio:/data bitnami/minio:latest

## rabbit https://hub.docker.com/_/rabbitmq

sudo docker run --restart unless-stopped -d --name rabbit -e RABBITMQ_DEFAULT_USER=myrabbit -e RABBITMQ_DEFAULT_PASS=???? -p5672:5672 -p15672:15672 -p15692:15692 \
-v /data/rabbit:/var/lib/rabbitmq \
-v /data/enabled_plugins:/etc/rabbitmq/enabled_plugins:ro \
rabbitmq:3-management

# TODO - mock web server

- kickstart account setup via POST msg and responds in real time
  - can be extended by third parties

1. commits account request msg to chain with transaction ID
2. waits for signal from chain
   - fail: account form invalid error msg
   - fail: chain error msg
   - success:
     - account form success msg AND
     - chain created on stores msg AND
     - chain created on msg broker
3. responds with success msg

# worker design patterns

- failover election pattern: job run by one worker with multiple failures
  1. workers read trigger msg
  2. workers emit their vote
  3. first worker processes msg
  4. if not done by timeout in trigger msg, next worker takes over
  5. when done, target worker emits results which tells other workers the election is over
- ...

Third party importers

- HTTP dump: dumps incoming http requests to chain (web hook payloads etc etc)
  - CLI
    - Appender
    - Response code
    - Response payload
    - IP restriction
    - Hash required
    - Asset store
    - HTTP or HTTPS
- FS dump: dumps file blobs to chain
  - CLI
    - Appender
    - Asset Store
    - Path
    - Include
    - Exclude
    - FileType
    - Size = <10M less then 10 megs
    - Owner=foo or Owner=123
    - ModifiedAt
    - CreatedAt
    - Catch file = tracks files it already processed
    - Persist = runs in background and checks folder for new items
- SQL dump: dumps data from sql db to chain
  - CLI
    - Host
    - Database
    - Table
    - Limit
    - Where
    - Paginate (multiple records per asset)
    - format=JSON,XML,YAML
- Mongo dump
  - CLI
    - Host
    - Database
    - Collection
    - Limit
    - Where
    - Paginate (multiple records per asset)
    - format=JSON,XML,YAML
- CSV dump: for breaking up large CSV files into smaller chunks to chain
  - CLI
    - Appender
    - Asset Store
    - FilePath
    - Delimiter
    - StartLineNo
    - EndLineNo
    - Paginate (multiple records per asset)
    - format=JSON,XML,YAML
    - IncludeHeaders
- Pipe Dump: dump pipped data to chain i.e. “echo testing | cmd”
  - CLI
    - Appender
    - Asset Store
- Stream dump: dumps streamed data like log files to chain
  - CLI
    - Appender
    - Asset Store
    - Sys event type
    - path = path to where ever the events are emitted from under /dev
    - Include = regex
    - Exclude = regex

In house custom oracles

- List contacts
- List orders
- List products
  - At a point in time i.e. 10/10/23
  - filter i.e. price >=10.99
  - Hide certain fields
  - Foreign key logic
    - UPSERT / INSERT / UPDATE / DELETE
  - Live update target
    - Track last committed block/asset IDs
  - Output to target data source
    - UPSERT / INSERT / UPDATE / DELETE
    - Output as CSV and import as CSV

Emitter CLI changes

- Each type of fake data is in it’s own named stream
- Named streams combined category streams
  - Round robin between each stream
  - Randomize order of src streams
- Main stream combined category streams
  - Round robin between each stream
  - Randomize order of src streams
- Main stream speed delay
- Find a big pile of json payloads and dump into a local dir
  - https://developer.imdb.com/non-commercial-datasets/
    - Run script to convert into complete json files
- Add stream for images
  - All local
  - MNIST data of hand writing https://keras.io/api/datasets/mnist/
  - CIFAR10 animals https://keras.io/api/datasets/cifar10/
  -
- Add stream for txt files
- Add stream for pdf files
- Add garbage stream that just generates buffers
- Echo msg to console when a stream is complete
- If appender can’t be reached echo clean error that appender not found in console.error
- If appender returns anything other than 201, return response msg txt in console.error
  —delay=200 wait 200ms between vals
  —max-size=120 max size of payload in kb

Web appender security solutions?

1. Rotating token

Browser RPC support

1. Browser POST to server
2. server generates session ID
3. server sends to appender
4. server walks chain looking for response session ID
5. Server returns payload to browser

- Server plugin responsibilities
  - generate session ID
  - walks chain
  - Reads assets
  - manages browser connections
- Server plugin args
  - Block store
  - Asset store
  - Walker sdk

heartbeat: appender/append/chain/{hash}

type: msg
createdAt: TS
from: asset ID of form
transID: UUID
success: false
msg: this form failed validation because whatever

type: msg
createdAt: TS
from: asset ID of form
transID: UUID
success: true
msg: form success!

type: form
id: new_account
createdAt: TS
transID: UUID
form: {
field1: foobar,
field2: testing
}
meta:
os: aaa
mem: 12GB
….

type: chain
id: hash
createdAt: TS
status: created

type: chain
id: hash
createdAt: TS
status: deleted

type: app
id: app_name
createdAt: TS
status: startup
meta:
os: aaa
mem: 12GB
….

type: app
id: app_name
createdAt: TS
status: update
meta: <- gets updated with every ping
foo: bar

type: app
id: app_name
createdAt: TS
status: shutdown
meta:
….

type: transaction
id: UUID
type: request
app: my_app
meta:
….

type: transaction
id: UUID
type: response
app: my_app
meta:
….

Msg types

- Chain maintenance
  - deboard
  - onboard
- Heartbeat
  - Up
    - App name
      - appender
      - builder
      - builderbroker
    - System
      - CPU
      - MEM
      - OS ver
      - DISK
  - Ping
    - builder
      - totalBlocks since start
  - Down
    - Reason (if available)
      - Self shutdown (builder detected a block with a next field)
      - User shutdown
      - Crash
      - Scheduled shutdown
      - Etc etc

2. Primary chain up and running
   1. Builder broker running (starts builder instance)
   2. Builder running
   3. Emitter sends fake data to chain
3. Secondary chain running
   1. Builder running + heartbeat to primary chain (started by builder broker)
   2. Emitter sends fake data to chain

util —asset-store=testing —block-store=testing —list-chains
Output example…
Chain: abcdef123 max height: 23423
Root asset data

# util —asset-store=testing —block-store=testing —chain=mychain —live

# B:BLOCK_ID H:HEIGHT

A:ASSET_ID #:1 TYPE:json/text
<<content goes here>>
—————————————-
A:ASSET_ID #:3 TYPE:jpg/image
binary
—————————————-

AWS example mock ups for training

https://lp.axoniq.io/hubfs/whitepaper%20DDD,%20CQRS%20and%20Event%20Sourcing%20Explained.pdf
https://hashingit.com/elements/research-resources/2017-02-16-dark-side-of-event-sourcing.pdf
http://chariotsolutions.com/wp-content/uploads/2015/05/ChrisRichardson_ETE2015_EventDrivenMicroservices.compressed.pdf

ChecksumConfirm: verifies pending checksums are on the chain

- Use by builder only
- Walks chain looking for passed checksums
- —checksum-ttl = amount of time to wait for a staged checksum to be a confirmed checksum
- If checksum hasn’t been confirmed, it’s re-submitted
- Requires
  - access to block store (gets from —block-store cli)
  - appender (gets from —appender cli)
- On startup. walk to tail while comparing checksums

(chain -> service)
QueueBroker:

- Trigger msgs
  - Chain discovery msg
- Output msgs
  - All raw responses from msg broker
  - Calls append(blob)
- What happens on failed attempt? loop retry (commit attempts)
- Walk to warm tail THEN apply changes
  - In Compliance: queues that are supposed to be created that are created
    - Has chain discovery msg and latest created queue msg is greater then latest deleted msg
  - Out of compliance: queues that are supposed to be created but don’t exist
    - Has chain discovery msg and latest created queue msg is greater then latest deleted msg
- After tail, apply changes as they happen
  1. Update compliance tables
  2. Send chain IDs to be created

(chain -> service)
BuilderBroker: manages many builders

- Trigger msgs
  - Chain discovery msg
- Populates chain tables
  - In compliance: chains that should be up and have active builders
  - Out compliance: chains that should be up but don’t have active builders
- Config input
  - max-active-age: n = number of minutes from now that a msg for a chain is within to count as being currently active
- build lists by traversing source chain
  - In Compliance: a hb msg within the uptime window
  - Out of compliance: a hb msg outside of

## ETL UTILITIES

### EXCEL -> chain

- batch by line #s
- can skip duplicates by walking chain
- commit file info and batch info
- can exclude columns
- can export embedded objects like formulas and scripts

### CSV -> chain

- batch by line #s
- can skip duplicates by walking chain
- commit file info and batch info
- can exclude columns

### DB -> chain

- specify table
- can exclude columns
- can batch by # of rows
- add where clause

### FILES -> chain

- specify dir
- can do recursive
- can add file meta data
- add size limit
- add date range
- add dir regex
- can skip duplicates by walking chain

### BUCKET -> chain

- commit manifest
- can skip duplicates by walking chain
- can add object meta data

CUSTOMER NEEDS

- Importing existing data
- External Oracles
  - Webhooks from cloud services
- Exporting chain data
- Detecting errors in chain
- helpers to integrate into existing apps
  - SDK walkers in different languages

User generated stuff

- key/vals from user submitted form
- Msg response i.e. form submission
  - Error
  - Success
-

# USE CASES FOR CAPTURING HOT DATA FROM REMOTE SOURCES

- How to capture data via API/w date range fields (cake use case)
  capture data in specific date range chunks. walk chain to track previous commits

* grab a custom range of remote data
  --block-store
  --asset-store
  --chain
  --from-ts
  --to-ts

* grab a custom range of remote data excluding some columns
  --block-store
  --asset-store
  --chain
  --from-ts
  --to-ts
  --exclude=last_name

* grab week 10 chunk of data
  --block-store
  --asset-store
  --chain
  --weekly
  --start-at=10 <- start at week 10

* grab weekly chunks of data over time starting at week 10 as a service
  --block-store
  --asset-store
  --chain
  --weekly
  --prevent-dups <- walk chain to find/skip existing chunks
  --start-at=10 <- start at week 10

# USE CASES FOR CAPTURING LOCAL LARGE FILES

- meta object attached to each asset

  - file path
  - file size in bits
  - asset ID of blob
  - batch blob checksum
  - asset size in bits
  - row range
  - batch # of #

- capture data from large local csv file
  --appender
  --block-store
  --asset-store
  --chain
  --path
  --rows=100 <- chunk every 100 rows
  --prevent-dups <- walk chain to find/skip existing chunks

- capture data from large local SQL dump file
  --appender
  --block-store
  --asset-store
  --chain
  --path
  --table <- specify table to use
  --prevent-dups <- walk chain to find/skip existing chunks

- capture data from a db instance data file like mysql or postgres
  --appender
  --block-store
  --asset-store
  --chain
  --path
  --table <- specify table to use
  --prevent-dups <- walk chain to find/skip existing chunks

# USE CASES FOR CAPTURING FROM DB INSTANCE WITH COLD DATA

- meta object attached to each asset

  - hostname
  - db
  - table
  - rows
  - asset ID of blob
  - batch blob checksum
  - asset size in bits
  - row range
  - batch # of #

- capture data from a mysql instance
  --appender
  --block-store
  --asset-store
  --chain
  --db <- specify db if unset, then all databases
  --table <- specify table to use
  --prevent-dups <- walk chain to find/skip existing chunks

- How to intergrade with current infrastructure
  - persistant service containers that commit data to chain
- How to capture all orders and order updates \* Followup issue: How to reduce wasted API calls
  init...

1. Complete order fetch by ORDER ID range in reverse
   Ongoing…
1. As orders are changes, do another fetch of order info for that Order ID and only append when diff from what’s cached

- Detect chargeback

  - order notes
  - email
  - user form
  - kount api

- Handling credit card processing from websites
  1. web server sends cc info to secrets manager
  2. web server sends secrets receipt to chain
  3. worker gets receipt from chain
  4. worker uses receipt to get cc info from secret manager
  5. worker processes cc info
  6. worker sends response from

How to detect missing data for event data I.e missing date ranges

CRM BIZ OBJECTS

- Contact
- Order
- Order Comments
- Phone Number
- Street Address
- Chargebacks
  - Order

Chargeback 911

- Email
- Order comments

Limelight

- Webhooks
  - Order created
  - Refund created
  - Order canceled
  - card declined
- API call
  - Order created
- CSV export

Kount

- Billing info

Merchant account

- Transactions
  - Last 4 of card

Cake

- CSV export
  - Visitor traffic
  - conversions

Walk chain showing everything
results

- BlockID / height
- AssetID / index
- Doc (image info, plain txt, unknown)

Walk chain skipping assets

- skip-assets

Show all assetIDs who’s blobs can’t be found after 3 failed attempts and a TTL of 500ms

- asset-retry=3 asset-ttl=500
  AssetID
  —————
  Hash
  …

Show count of all assetIDs grouped by assetID in desc order limit 10

- Pipe command into linux reporting utilities like grep and sed
  AssetID | total
  ———————
  Hash | 41
  …

Walk chain without block validation. Walks without checking anything

- disable-validation

Walk chain ignoring all block validation (validates but continues anyway)

- skip-validation

Walk chain ignoring NEXT field validation (validates but continues anyway)

- skip-next-validation

Walk chain showing assets with image info. Skips all non-images
results

- Asset ID / Index
- Block ID / height
- Image info

Walk chain ignoring all validation checks

- ignore-all

(pending) Show count of all blocks across all block stores
BlockID | total
———————
Hash | 3

chain utility: walk chain showing asset info

- ## Walk to live showing blocks
- Walk to a segment
  - from=bid to=bid

Asset utility: walk chain showing raw assets
parser utility

- Show all results for assets that do match an asset parser
  - parser=foobar invert
- Show all results for assets that do match an asset parser
  - parser=foobar
- Show all results for an asset when passed through all parsers
  - parser=\* asset=assedID
- Get results of an asset when passed through a parser
  - parser=foobar asset=assedID
- Get results of an asset when passed through 2 parsers
  - parser=foobar,foobar2 asset=assedID
- Get results of all assets using generic parser (output parsed doc) and limit results
  - from=blockId to=blockId
  - from=height to=height
- Get results of all assets using matching query
  - query=.bla\*
- Get results of all assets that don’t match the query
  - query=.bla\* invert
- Results
  - BlockID: height
  - AssetID: idx
  - Parsed results

For each blob, apply asset parser. On match

How to run two streams in a pipeline at the same time i.e. Promise.all

- Might help with .connect() code
- Might help with running parallel calls in a pipeline
  forkJoin(of('a'), of('b'))
  .subscribe((v)=>{
  console.log(v)
  })

How to expose a secondary variable to a section of the pipeline
of('a')
.pipe(
concatMap((v)=>{
return interval(1000)
.pipe(
map(x=>{
return x + 'test' + v
}),
take(6)
)
})
)
.subscribe((v)=>{
console.log(v)
})

Simple English Gematria

Smart Contract logic

- Collect matching assets on traversal
  - BlockID / asset idx
- After each match, determine if an event can be generated
- On matching event: pass event to lambda
  - Event Name
  - Event Data
  - (Possibly) Block ID / asset indexes
  - (Possibly) purge matched assets

Smart Contract objects

- Asset Filter: func to determine if blob is a match
- Event: defines the name of the event and asset filters involved
- Event Meta Data
- Lambda: returns promise as signal

Event msg: can be sorted by (max height)-(max asset idx)-EVENT_NAME

- name: EVENT_NAME
- assets: [{}]
  - blockid
  - height
  - assetID IDX
- meta: {…}

Event lambda

Smart Contract Examples

- ORDER_CREATED
  - - Cart items
  - Order created
- ORDER_PROCESSED
  - - Cart items
  - Purchase item
- ORDER_CANCELED
- ACCOUNT\_

Stability issues

- Worker doesn’t reconnect when dependent service restarts
- Detached chains need to be manually re-attached using patchblock script
- Packer has to use static chain

Stability solutions

- Sidecar container for packer to select chain to attach to
- Sidecar container for PatchBlock dynamically picks chain to attach to

Example1-5
Client.append (multiple)(chain)

- Could pack checksums[] instead of packer service
- Also works for preserving checksum order
  Appender (multiple)(no chain)
  Packer (multiple)(chain)
- May not need if client can do this
  Builder (singleton)(chain)
- Walks to tail
- Builds block
- Updates next field of prev block. \* If next field already exists, then critical exit
  Deploy (multiple)(no chain)
- Pops block from queue and sends to block store
  PatchBlock
- Used for fixing broken chains
- Used for generating root block
- Possibly used for creating root block via CLI with automation

BLOCKSTORE_URL
ASSETSTORE_URL
MSGBROKER_URL

Routine to search for tail block

- Requires
  - blockStore
  - chain
- Search all block stores for blocks with missing next field
- Returns block with the biggest height

Routine to list chains

- Requires
  - Msg broker
- List queues
- Returns array of chains

Routine to check block builder is working

Routine to tell if chain is detached: pending assets staged but no blocks are coming out

- Requires
  - blockStore
  - chain
- ?

Routine to check if block builder is active for a chain

Routine to Reattach chain

- Exit if: pending assets queue is empty
- Exit if: pending tail block not available
-

APPENDER USE CASES:

- Capturing system logs
  - Many small pieces of text
  - Order is required
- RPC commands i.e. direct chat
  - Fewer medium JSON docs
  - Retransmissions have to be tracked
  - Wait for reply JSON doc on chain
- Webhook events
  - Many medium sized docs
  - Every one is important
- GPS point tracking
  - Lots of noise, little signal
  - Capture certain points

APPENDER:

- Build big chunks of small blobs i.e. sys logs
  new Appender(assetStore, worker, chain, auth, confirm, limit=100, ttl=4000)

* app.appendPage(txt)

-

* app.append(blob)

- Listen for all msgs

* app.watch((v)=>{

- Confirm it made it to asset store
  - Asset store ID or ip address
- Confirm it made it to chain (if specified)
  - block ID
  - AssetID pos
- Round trip time
  })

Lifecycle of value object
{
chain: str
}
{
chain: str
checksum: str
}
{
chain: str
checksum: str
error: str
}

confirmToChain = new ChainConfirmStream(walker)

myLogWatcher = new LogWatchStream()

myAppenderStream = new AppenderStream(TTL, retries)

1. Sends val object to appender

myASStream.subscribe((v)=>{

myAppenderStream.next({
checksum
chain
})

})

myAppenderStream.subscribe((v)=>{
confirmToChain({
checksum
chain
})
}

IngressAssetStoreStream: inserts into asset store

- Schema checks v
- Infinite retries on error
- Config TTL between retries

IngressAppenderStream: Sends checksum to appender

- Schema checks v
- Infinite retries on error
- Config TTL between retries

EgressStream: Emits vals to promise or whatever outside the app

VerifyChainArray: ordered by age of checksum

- When 0 length, get tail block as starting block

Walker

- Starts from root block
- For each block
  - Check each item in VerifyChainArray against it

Things to watch out for

- Access denied error from asset store
  - Emit general timed loop error until resolved
- Access denied error from appender
  - Emit general timed loop error until resolved
- Disk full from asset store
  - Emit general timed loop error until resolved
- VerifyChainArray too big
-
-

1. Upload to asset store
2. Send to appender
3. Send to local cache

Asset confirm: confirm asset is in asset store

1. gen checksum
2. Insert blob into asset store

Block confirm: confirm asset checksum is in block chain

1. Append checksum
2. Walk chain looking for the checksum

file:///my/bla
/my/bla
mongodb+srv://mycluster.com/mydb
s3.my_bucket.us-east-1.amazonaws.com
http://my_bucket.s3.amazonaws.com/
http://s3.amazonaws.com/my_bucket

Driver type detect rules

1. By url
2. By port number
3. Port response

Store url to driver steps

1. Detect driver type, on fail throw error
2. Use driver specific parser on url, on parse fail, throw error
3. If any required fields missing, throw error

asset store -
.get(id): return buffer
.create(id,blob): return {id} - fail if it exists
.put(id,blob): return {id} - overwrite if needed

Block store -
.get(id): return block
.create(block): return true - success if block doesn’t exist
.updateNext(blockId, nextId) - success if block exists with an unset next field

Tick counter

Account creator
Chain creator
Service account creator

ACTIONS: modify system

- returns: JSON of results to be appended to chain, error or success
- Event schema template
  {
  host: hostname
  Ip: ip address
  createdTS: timestamp
  data: {}
  }

DEMO SCRIPTS:
appendblobs.js: appends contacts, addresses, phones, comments etc etc to appender

- chain

Echo events detected on a chain
walkevents.js

deleteaccount(accountID)
{
accountId:
}

createservicetoken(accountID)
deleteservicetoken(accountID, servicetokenID)

asset seeking

----2----5----3-----------0-----------4----1---6---
